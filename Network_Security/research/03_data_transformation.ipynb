{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "import os \n",
    "from datetime import datetime \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Data_Ingestion_Artifact:\n",
    "    train_file_path:str \n",
    "    test_file_path:str\n",
    "\n",
    "@dataclass\n",
    "class Data_validation_Artifact:\n",
    "    validation_status:bool\n",
    "    message: str\n",
    "    drift_report_file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationArtifact:\n",
    "    transformed_object_file_path:str \n",
    "    transformed_train_file_path:str\n",
    "    transformed_test_file_path:str\n",
    "\n",
    "# Artifacts\n",
    "ARTIFACTS = 'artifacts'\n",
    "PIPELINE_DIR = 'network'\n",
    "@dataclass \n",
    "class NS_Train_Configeration:\n",
    "    artifact_dir:str = ARTIFACTS\n",
    "    pipeline_dir:str = PIPELINE_DIR\n",
    "    TIMESTAMP = datetime.now().strftime('%m_%d_%Y_%H_%M_%S')\n",
    "\n",
    "train_config = NS_Train_Configeration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf468ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRANSFORMATION_DIR:str = 'data_transformation'\n",
    "DATA_TRANSFORMATION_TRANSFORM_FILE:str = 'transform'\n",
    "DATA_TRANSFORMATION_OBJECT_DIR:str = 'transform_object' \n",
    "\n",
    "# data\n",
    "RAW_DATA = 'raw.csv'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "TEST_DATA = 'test.csv'\n",
    "PREPROCESSOR_FILE = 'preprocessor.pkl'\n",
    "TARGET_COLUMN = '------------------------------------------'\n",
    "CURRENT_DATE = datetime.now()\n",
    "\n",
    "class Data_Transformation_config:\n",
    "    data_transformation_dir = os.path.join(train_config.artifact_dir,DATA_TRANSFORMATION_DIR)\n",
    "    data_transformation_train_file = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_TRANSFORM_FILE,TRAIN_DATA.replace('csv','npy'))\n",
    "    data_transformation_test_file = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_TRANSFORM_FILE,TEST_DATA.replace('csv','npy'))\n",
    "    data_transformation_object = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_OBJECT_DIR,PREPROCESSOR_FILE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e793bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "def read_yaml_file(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'rb') as file:\n",
    "        yaml.safe_load(file) \n",
    "\n",
    "def read_data(df:pd.DataFrame):\n",
    "    return pd.read_csv(df)\n",
    "\n",
    "def drop_col(df:pd.DataFrame,col):\n",
    "    return df.drop(columns=[col],axis=1,inplace=True)\n",
    "\n",
    "class TargetValueMapping:\n",
    "    def __init__(self):\n",
    "        self.male = 0\n",
    "        self.female = 1 \n",
    "    def _asdict(self):\n",
    "        return self.__dict__ \n",
    "    def reverse(self):\n",
    "        return dict(zip(self._asdict().values(), self._asdict().keys()))\n",
    "    \n",
    "def save_numpy_array(file_path: str, array: np.array):\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(file_path, 'wb') as file_obj:\n",
    "            np.save(file_obj, array)\n",
    "\n",
    "def save_object(file_path: str,obj):\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(file_path, 'wb') as file_obj:\n",
    "            dill.dump(file_obj,obj)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Network_Security.constant import SEHEMA_FILE_PATH\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler,PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "class Data_Transformation:\n",
    "    def __init__(self,data_ingestion_artifact=Data_Ingestion_Artifact,\n",
    "                 data_validation_artifact=Data_validation_Artifact,\n",
    "                 data_transformation_config=Data_Transformation_config):\n",
    "        self.data_ingestion_artifact = data_ingestion_artifact\n",
    "        self.data_validation_artifact= data_validation_artifact\n",
    "        self.data_transformation_config= data_transformation_config\n",
    "        self._sehema = read_yaml_file(file_path=SEHEMA_FILE_PATH)\n",
    "    def get_data_transformation(self):\n",
    "        ohe_transform = OneHotEncoder()\n",
    "        or_transform = OrdinalEncoder()\n",
    "        scaler = StandardScaler()\n",
    "        pw_transform = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "        ohe_col = self._sehema['ohe_columns']\n",
    "        or_col = self._sehema['or_columns']\n",
    "        num_col = self._sehema['numerical_columns']\n",
    "        power_tf_col = self._sehema['transform_columns']\n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('OneHotEncoder',ohe_transform,ohe_col),\n",
    "            ('OrdinalEncoder',or_transform,or_col),\n",
    "            ('PowerTransformer',pw_transform,power_tf_col),\n",
    "            ('StandardScaler',scaler,num_col)\n",
    "\n",
    "        ])\n",
    "        return preprocessor \n",
    "    def init_data_transformation(self):\n",
    "        if self.data_validation_artifact.validation_status:\n",
    "            preprocessor = self.get_data_transformation()\n",
    "            train_df = Data_Transformation.read_data(self.data_ingestion_artifact.train_file_path)\n",
    "            test_df = Data_Transformation.read_data(self.data_ingestion_artifact.test_file_path)\n",
    "\n",
    "            xtrain = train_df.drop(columns=[TARGET_COLUMN],axis=1)\n",
    "            ytrain = train_df[TARGET_COLUMN]\n",
    "\n",
    "            xtrain['company_age'] = CURRENT_DATE - xtrain['company_estabilish']\n",
    "            drop_col = self._sehema['drop_col']\n",
    "            xtrain=drop_col(xtrain,drop_col)\n",
    "\n",
    "            ytrain = ytrain.replace(TargetValueMapping()._asdict())\n",
    "\n",
    "            #test-df\n",
    "            xtest = test_df.drop(columns=[TARGET_COLUMN],axis=1)\n",
    "            ytest = test_df[TARGET_COLUMN]\n",
    "\n",
    "            xtest['company_age'] = CURRENT_DATE - xtest['company_estabilish']\n",
    "            drop_col = self._sehema['drop_col']\n",
    "            xtest=drop_col(xtest,drop_col)\n",
    "\n",
    "            ytest = ytest.replace(TargetValueMapping()._asdict())\n",
    "\n",
    "            xtrain_arr = preprocessor.fit_transform(xtrain)\n",
    "            xtest_arr = preprocessor.transform(xtest)\n",
    "\n",
    "            from imblearn.combine import SMOTEENN \n",
    "            smt = SMOTEENN(sampling_strategy=\"minority\")\n",
    "            xtrain_arr,ytrain = smt.fit_resample(xtrain_arr,ytrain)\n",
    "\n",
    "            smt = SMOTEENN(sampling_strategy=\"minority\")\n",
    "            xtest_arr,ytest = smt.fit_resample(xtest_arr,ytest)\n",
    "\n",
    "            train_arr = np.c_[xtrain_arr,np.array(ytrain)]\n",
    "            test_arr = np.c_[xtest_arr,np.array(ytest)]\n",
    "\n",
    "            save_object(self.data_transformation_config.data_transformation_object,preprocessor)\n",
    "            save_numpy_array(self.data_transformation_config.data_transformation_train_file,array=train_arr)\n",
    "            save_numpy_array(self.data_transformation_config.data_transformation_train_file,array=test_arr)\n",
    "\n",
    "            return DataTransformationArtifact(transformed_object_file_path= self.data_transformation_config.transformed_object_file_path,\n",
    "                                                transformed_train_file_path= self.data_transformation_config.transformed_train_file_path,\n",
    "                                                transformed_test_file_path= self.data_transformation_config.transformed_test_file_path\n",
    "                                                )\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ac510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_Pipeline:\n",
    "    def __init__(self):\n",
    "    #   self.data_ingestion_config = Data_ingestion_Config()\n",
    "    #   self.validation_config = Data_validation_config()\n",
    "        self.data_transformation_config= Data_Transformation_config()\n",
    "\n",
    "\n",
    "\n",
    "    # def start_data_ingestion(self)->Data_Ingestion_Artifact:\n",
    "    #     data_ingestion = Data_Ingestion(ingestion_config=self.data_ingestion_config)\n",
    "    #     data_ingestion_artifacet = data_ingestion.init_data_ingestion()\n",
    "    #     return data_ingestion_artifacet\n",
    "\n",
    "    # def start_data_validation(self,data_ingestion_artifacet:Data_Ingestion_Artifact)-> Data_validation_Artifact:\n",
    "    #     data_valid = Data_validation(data_ingestion_artifacet=data_ingestion_artifacet,\n",
    "    #                                   data_validation_config=self.data_validation_config)\n",
    "    #     data_validation_Artifact = data_valid.init_data_ingestion()\n",
    "    #     return data_validation_Artifact\n",
    "\n",
    "    def start_data_transform(self,data_ingestion_artifacet:Data_Ingestion_Artifact,\n",
    "                             data_validation_Artifact:Data_validation_Artifact)->DataTransformationArtifact:\n",
    "        data_transform_config = Data_Transformation(\n",
    "                                                    data_ingestion_artifacet=data_ingestion_artifacet,\n",
    "                                                    data_validation_Artifact=data_validation_Artifact,\n",
    "                                                    ata_transformation_config=Data_Transformation_config\n",
    "                                                    )\n",
    "        data_transform_artifact = data_transform_config.init_data_transformation()\n",
    "        return data_transform_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83022256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_data_ingestion(self) -> DataIngestionArtifact:\n",
    "        \"\"\"\n",
    "        This method of TrainPipeline class is responsible for starting data ingestion component\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Entered the start_data_ingestion method of TrainPipeline class\")\n",
    "            logging.info(\"Getting the data from mongodb\")\n",
    "            data_ingestion = DataIngestion(data_ingestion_config=self.data_ingestion_config)\n",
    "            data_ingestion_artifact = data_ingestion.initiate_data_ingestion()\n",
    "            logging.info(\"Got the train_set and test_set from mongodb\")\n",
    "            logging.info(\n",
    "                \"Exited the start_data_ingestion method of TrainPipeline class\"\n",
    "            )\n",
    "            return data_ingestion_artifact\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys) from e\n",
    "        \n",
    "\n",
    "    \n",
    "def start_data_validation(self, data_ingestion_artifact: DataIngestionArtifact) -> DataValidationArtifact:\n",
    "        \"\"\"\n",
    "        This method of TrainPipeline class is responsible for starting data validation component\n",
    "        \"\"\"\n",
    "        logging.info(\"Entered the start_data_validation method of TrainPipeline class\")\n",
    "\n",
    "        try:\n",
    "            data_validation = DataValidation(data_ingestion_artifact=data_ingestion_artifact,\n",
    "                                             data_validation_config=self.data_validation_config\n",
    "                                             )\n",
    "\n",
    "            data_validation_artifact = data_validation.initiate_data_validation()\n",
    "\n",
    "            logging.info(\"Performed the data validation operation\")\n",
    "\n",
    "            logging.info(\n",
    "                \"Exited the start_data_validation method of TrainPipeline class\"\n",
    "            )\n",
    "\n",
    "            return data_validation_artifact\n",
    "\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys) from e\n",
    "        \n",
    "    \n",
    "def start_data_transformation(self, data_ingestion_artifact: DataIngestionArtifact, data_validation_artifact: DataValidationArtifact) -> DataTransformationArtifact:\n",
    "        \"\"\"\n",
    "        This method of TrainPipeline class is responsible for starting data transformation component\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_transformation = DataTransformation(data_ingestion_artifact=data_ingestion_artifact,\n",
    "                                                     data_transformation_config=self.data_transformation_config,\n",
    "                                                     data_validation_artifact=data_validation_artifact)\n",
    "            data_transformation_artifact = data_transformation.initiate_data_transformation()\n",
    "            return data_transformation_artifact\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
