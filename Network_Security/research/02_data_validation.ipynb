{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d71e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "from datetime import datetime\n",
    "import os \n",
    "import sys \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Data_Ingestion_Artifact:\n",
    "    train_file_path:str \n",
    "    test_file_path:str\n",
    "\n",
    "@dataclass \n",
    "class Data_validation_Artifact:\n",
    "    validation_status:bool \n",
    "    message_error:str \n",
    "    drift_report_file_path:str\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76162347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifacts\n",
    "ARTIFACTS = 'artifacts'\n",
    "PIPELINE_DIR = 'network'\n",
    "\n",
    "@dataclass \n",
    "class NS_Train_Configeration:\n",
    "    artifact_dir:str = ARTIFACTS\n",
    "    pipeline_dir:str = PIPELINE_DIR\n",
    "    TIMESTAMP = datetime.now().strftime('%m_%d_%Y_%H_%M_%S')\n",
    "\n",
    "train_config = NS_Train_Configeration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3abc2aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_VALIDATION_DIR:str = 'data_validation'\n",
    "DATA_VALIDATION_REPORT_DIR:str = 'drift_report'\n",
    "DATA_VALIDATION_REPORT_YAML:str = 'report.yaml'\n",
    "\n",
    "\n",
    "class Data_validation_config:\n",
    "    data_validation_dir = os.path.join(train_config.artifact_dir,DATA_VALIDATION_DIR)\n",
    "    data_validation_report = os.path.join(data_validation_dir,DATA_VALIDATION_REPORT_DIR,DATA_VALIDATION_REPORT_YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe4159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "def read_yaml_file(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'rb') as file:\n",
    "        yaml.safe_load(file) \n",
    "    \n",
    "\n",
    "\n",
    "def write_yaml_file(file_path: str, content: object, replace: bool = False) -> None:\n",
    "    \n",
    "        if replace:\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)   \n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)  \n",
    "        with open(file_path, \"w\") as file:\n",
    "            yaml.dump(content, file)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.constant import SEHEMA_FILE_PATH\n",
    "from evidently.model_profile import Profile\n",
    "from evidently.model_profile.sections import DataDriftProfileSection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Network_Security.components.data_ingestion import DataIngestion\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "class Data_validation:\n",
    "    def __init__(self,data_ingestion_artifact=Data_Ingestion_Artifact,\n",
    "                data_validation_config=Data_validation_config):\n",
    "        self.data_ingestion_artifact = data_ingestion_artifact\n",
    "        self.data_validation_config = data_validation_config \n",
    "        self._sehema_yaml = read_yaml_file(file_path=SEHEMA_FILE_PATH)\n",
    "\n",
    "    def valid_no_columns(self,dataframe: pd.DataFrame)->bool:\n",
    "        status =  len(dataframe.columns) == self._sehema_yaml['columns'] \n",
    "        return status \n",
    "    \n",
    "    def Is_column_exists(self,dataframe=pd.DataFrame)->bool:\n",
    "        missing_num_columns = []\n",
    "        for column in self._sehema_yaml['numeric_columns']:\n",
    "            if column not in dataframe.columns:\n",
    "                missing_num_columns.append(column)\n",
    "            if len(missing_num_columns)>0:\n",
    "                logging.info('Missing numeric column',missing_num_columns)\n",
    "\n",
    "        missing_cat_columns = []\n",
    "        for column in self._sehema_yaml['categorical_columns']:\n",
    "            if column not in dataframe.columns:\n",
    "                missing_cat_columns.append(column)\n",
    "            if len(missing_cat_columns)>0:\n",
    "                logging.info('Missing categorical column',missing_cat_columns)\n",
    " \n",
    "        status =  [False if len(missing_num_columns) or len(missing_cat_columns) > 0 else True]\n",
    "        return status\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_data(dataframe)->pd.DataFrame:\n",
    "        return pd.read_csv(dataframe)\n",
    "\n",
    "    def detect_dataset_drift(self,reference_df:pd.DataFrame,current_df:pd.DataFrame):\n",
    "        #detect_drift_profile = Profile(sections=[DataDriftProfileSection()])\n",
    "        detect_drift_profile = Profile(DataDriftProfileSection())\n",
    "        detect_drift_profile.calculate(reference_df,current_df)\n",
    "        report = detect_drift_profile.json()\n",
    "        json_report = json.loads(report)\n",
    "\n",
    "        write_yaml_file(file_path=self.data_validation_config.data_validation_report,content=json_report)\n",
    "\n",
    "        n_feature = json_report['data_drift']['data']['metrics']['n_feature']\n",
    "        n_drift_feature = json_report['data_drift']['data']['metrics']['n_dragt_feature']\n",
    "        logging.info(f\"{n_drift_feature}/{n_feature} drift detected.\")\n",
    "        drift_status = json_report[\"data_drift\"][\"data\"][\"metrics\"][\"dataset_drift\"]\n",
    "        return drift_status\n",
    "    \n",
    "    def init_data_validation(self)-> Data_Ingestion_Artifact:\n",
    "        valid_message_error = []\n",
    "        train_data,test_data = DataIngestion.read_data(self.data_ingestion_artifact.train_file_path,\n",
    "                                                        self.data_ingestion_artifact.test_file_path)\n",
    "        # train_data\n",
    "        status = self.valid_no_columns(train_data)\n",
    "        if not status:\n",
    "            valid_message_error += 'Error: Column Mismatch'\n",
    "        \n",
    "        status = self.Is_column_exists(train_data)\n",
    "        if not status:\n",
    "            valid_message_error += 'Error: Column Mismatch'\n",
    "        # test_data\n",
    "        status = self.valid_no_columns(train_data)\n",
    "        if not status:\n",
    "            valid_message_error += 'Error: Column Mismatch'\n",
    "        \n",
    "        status = self.Is_column_exists(train_data)\n",
    "        if not status:\n",
    "            valid_message_error += 'Error: Column Mismatch'\n",
    "        # drift_detect\n",
    "        validation_status = len(valid_message_error)== 0\n",
    "        if validation_status:\n",
    "            status = self.detect_dataset_drift(train_data,test_data)\n",
    "            if status:\n",
    "                valid_message_error = 'Drift detected'\n",
    "            else:\n",
    "                valid_message_error = 'Drift not detected'\n",
    "        else:\n",
    "            logging.info(f'valid_message_error{valid_message_error}')\n",
    "           \n",
    "        data_validation_Artifact = Data_validation_Artifact(\n",
    "            validation_status= validation_status,\n",
    "            message_error=valid_message_error,\n",
    "            drift_report_file_path=self.data_validation_config.data_validation_report\n",
    "        )\n",
    "        return data_validation_Artifact\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d944dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_Pipeline:\n",
    "    def __init__(self):\n",
    "    #   self.data_ingestion_config = Data_ingestion_Config()\n",
    "        self.validation_config = Data_validation_config()\n",
    "\n",
    "\n",
    "    # def start_data_ingestion(self)->Data_Ingestion_Artifact:\n",
    "    #     data_ingestion = Data_Ingestion(ingestion_config=self.data_ingestion_config)\n",
    "    #     data_ingestion_artifacet = data_ingestion.init_data_ingestion()\n",
    "    #     return data_ingestion_artifacet\n",
    "\n",
    "    def start_data_validation(self,data_ingestion_artifacet:Data_Ingestion_Artifact)-> Data_validation_Artifact:\n",
    "        data_valid = Data_validation(data_ingestion_artifacet=data_ingestion_artifacet,\n",
    "                                      data_validation_config=self.data_validation_config)\n",
    "        data_validation_Artifact = data_valid.init_data_ingestion()\n",
    "        return data_validation_Artifact\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
