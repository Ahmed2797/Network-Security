{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34cb1bf5",
   "metadata": {},
   "source": [
    "# DataIngestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b481a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np \n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DATABASE_NAME = 'NETWORK_SECURITY'\n",
    "COLLECTION_NAME = 'NETWORK_DATA' \n",
    "MONGODB_URL = 'MONGODB_URL'\n",
    "\n",
    "ARTIFACTS_DIR = 'artifacts'\n",
    "PIPELINE_DIR = 'security'\n",
    "\n",
    "DATA_INGESTION_DIR_NAME : str = 'data_ingestion'\n",
    "DATA_INGESTION_COLLECTION_NAME: str = 'NETWORK_DATA'\n",
    "DATA_INGESTION_FEATURE_STORED_NAME:str = 'feature'\n",
    "DATA_INGESTION_INGESTED_NAME:str = 'ingested'\n",
    "DATA_INGESTION_SPLIT_RATIO:float = 0.2 \n",
    "\n",
    "RAW_DATA = 'security.csv'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "TEST_DATA = 'test.csv'\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime('%m_%d_%Y_%H_%M_%S')\n",
    "\n",
    "@dataclass \n",
    "class TrainingConfiguration:\n",
    "    artifact_dir:str = ARTIFACTS_DIR \n",
    "    piprline_dir:str = PIPELINE_DIR \n",
    "    timestamp:str = TIMESTAMP \n",
    "\n",
    "trainingconfig : TrainingConfiguration=TrainingConfiguration()\n",
    "\n",
    "@dataclass \n",
    "class Data_ingestion_configeration:\n",
    "    data_ingestion_dir:str = os.path.join(trainingconfig.artifact_dir,DATA_INGESTION_DIR_NAME)\n",
    "    data_ingestion_collection:str = DATA_INGESTION_COLLECTION_NAME \n",
    "    data_ingestion_feature:str = os.path.join(data_ingestion_dir,DATA_INGESTION_FEATURE_STORED_NAME,RAW_DATA)\n",
    "    train_data_path:str = os.path.join(data_ingestion_dir,DATA_INGESTION_INGESTED_NAME,TRAIN_DATA)\n",
    "    test_data_path:str = os.path.join(data_ingestion_dir,DATA_INGESTION_INGESTED_NAME,TEST_DATA)\n",
    "    split_ratio:float = DATA_INGESTION_SPLIT_RATIO\n",
    "\n",
    "\n",
    "\n",
    "# mongodb\n",
    "from Network_Security.constant import MONGOBD_URL, DATA_BASE_NAME\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "from Network_Security.logging.logger import logging\n",
    "from dotenv import load_dotenv \n",
    "import certifi \n",
    "import pymongo\n",
    "import sys\n",
    "import os \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "# MONGOBD_URL = os.getenv(\"MONGOBD_URL\")\n",
    "ca = certifi.where()  \n",
    "\n",
    "class MongoDBClient:\n",
    "    def __init__(self, database=DATA_BASE_NAME):\n",
    "        try:\n",
    "            mongo_url = os.getenv(MONGOBD_URL)\n",
    "            if mongo_url is None:\n",
    "                logging.info(\"MongoDB URL not found in environment variables\")\n",
    "                raise ValueError(\"MongoDB URL is missing\")\n",
    "\n",
    "            MongoDBClient.client = pymongo.MongoClient(mongo_url, tlsCAFile=ca)\n",
    "            self.client = MongoDBClient.client \n",
    "            self.database = self.client[database]\n",
    "            self.database_name = database  \n",
    "\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    " \n",
    "\n",
    "#networkdata_acces\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.configeration.mongodb import MongoDBClient \n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sys\n",
    "\n",
    "\n",
    "class NetworkData:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.mongo_client = MongoDBClient()   \n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "\n",
    "    def get_dataframe(self, collection_name: str, database_name: Optional[str] = None)->pd.DataFrame:\n",
    "        try:\n",
    "            if database_name:\n",
    "                collection = self.mongo_client.client[database_name][collection_name]\n",
    "            else:\n",
    "                collection = self.mongo_client.database[collection_name]\n",
    "\n",
    "            df = pd.DataFrame(list(collection.find()))\n",
    "            if \"_id\" in df.columns:\n",
    "                df.drop(columns=[\"_id\"], inplace=True)\n",
    "            df.replace(\"na\", np.nan, inplace=True)\n",
    "\n",
    "            logging.info(\"DataFrame Extract Successful\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "\n",
    "\n",
    "# @dataclass \n",
    "# class TrainingPipelineConfig:\n",
    "#     TRAIN_DATA_PATH:str \n",
    "#     TEST_DATA:str \n",
    "\n",
    "# data_ingestion\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.entity.config import Data_ingestion_Config\n",
    "from Network_Security.entity.artifact import Data_Ingestion_Artifact\n",
    "from Network_Security.configeration.mongodb import MongoDBClient  \n",
    "from Network_Security.data_acess.networkdata_acess import NetworkData \n",
    "\n",
    "class Data_Ingestion:\n",
    "    def __init__(self, ingestion_config: Data_ingestion_Config):\n",
    "        try:\n",
    "            self.ingestion_config = ingestion_config\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "\n",
    "    def get_feature_extract_data(self):\n",
    "        try:\n",
    "            logging.info(\"Extracting data from MongoDB...\")\n",
    "            networkdata = NetworkData()\n",
    "            \n",
    "            dataframe = networkdata.get_dataframe(\n",
    "                collection_name=self.ingestion_config.data_ingestion_collection_path\n",
    "            )\n",
    "            # start feature_store\n",
    "            feature_data_path = self.ingestion_config.data_ingestion_feature_path\n",
    "            os.makedirs(os.path.dirname(feature_data_path), exist_ok=True)\n",
    "            dataframe.to_csv(feature_data_path, index=False, header=True)\n",
    "            logging.info(f\"Data stored at {feature_data_path}\")\n",
    "            return dataframe\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "\n",
    "    def split_data(self, dataframe: pd.DataFrame):\n",
    "        try:\n",
    "            train_data, test_data = train_test_split(\n",
    "                dataframe, \n",
    "                test_size=self.ingestion_config.split_ratio\n",
    "            )\n",
    "\n",
    "            train_file_path = self.ingestion_config.train_data_path\n",
    "            os.makedirs(os.path.dirname(train_file_path), exist_ok=True)\n",
    "            train_data.to_csv(train_file_path, index=False, header=True)\n",
    "\n",
    "            test_file_path = self.ingestion_config.test_data_path\n",
    "            os.makedirs(os.path.dirname(test_file_path), exist_ok=True)\n",
    "            test_data.to_csv(test_file_path, index=False, header=True)\n",
    "\n",
    "            logging.info(\"Train & Test datasets saved successfully.\")\n",
    "            return train_data, test_data\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "\n",
    "    def init_data_ingestion(self):\n",
    "        try:\n",
    "            dataframe = self.get_feature_extract_data()\n",
    "            print(dataframe.head())\n",
    "            self.split_data(dataframe)\n",
    "\n",
    "            data_ingestion_artifact = Data_Ingestion_Artifact(\n",
    "                train_file_path=self.ingestion_config.train_data_path,\n",
    "                test_file_path=self.ingestion_config.test_data_path\n",
    "            )\n",
    "            logging.info(\"Data Ingestion completed successfully.\")\n",
    "            return data_ingestion_artifact\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys) \n",
    "        \n",
    "\n",
    "from Network_Security.components.data_ingestion import Data_Ingestion\n",
    "from Network_Security.components.data_validation import Data_validation\n",
    "from Network_Security.entity.config import (Data_ingestion_Config,\n",
    "                                            Data_validation_config) \n",
    "from Network_Security.entity.artifact import (Data_Ingestion_Artifact,\n",
    "                                              Data_validation_Artifact)\n",
    "\n",
    "\n",
    "\n",
    "class Training_Pipeline:\n",
    "    def __init__(self):\n",
    "        self.data_ingestion_config = Data_ingestion_Config()\n",
    "\n",
    "    def start_data_ingestion(self)->Data_Ingestion_Artifact:\n",
    "        data_ingestion = Data_Ingestion(ingestion_config=self.data_ingestion_config)\n",
    "        data_ingestion_artifact = data_ingestion.init_data_ingestion()\n",
    "        return data_ingestion_artifact \n",
    "\n",
    "    def run_pipeline(self)->None:\n",
    "        data_ingestion_artifact = self.start_data_ingestion()\n",
    "        \n",
    "\n",
    "        return None   \n",
    "\n",
    "from Network_Security.pipeline.train_pipeline import Training_Pipeline\n",
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "import sys \n",
    "\n",
    "if __name__ == '__main__':\n",
    "        logging.info('Starting Training Pipeline...')\n",
    "        pipeline = Training_Pipeline()\n",
    "\n",
    "        # Data Ingestion\n",
    "        logging.info('>>> Starting Data Ingestion')\n",
    "        data_ingestion_artifact = pipeline.start_data_ingestion()\n",
    "        logging.info(f'>>> Data Ingestion Completed: {data_ingestion_artifact}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5203ab0",
   "metadata": {},
   "source": [
    "# Data_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ead96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant.__init__.py\n",
    "DATA_VALIDATION_DIR:str = 'data_validation'\n",
    "DATA_VALIDATION_REPORT_DIR:str = 'drift_report'\n",
    "DATA_VALIDATION_REPORT_YAML:str = 'report.yaml'\n",
    "SEHEMA_FILE_PATH = os.path.join('data_schema','column.yaml')\n",
    "\n",
    "# config.py\n",
    "from dataclasses import dataclass \n",
    "from datetime import datetime\n",
    "from Network_Security.constant import *\n",
    "TIMESTAMP = datetime.now().strftime('%m_%d_%Y_%H_%M_%S')\n",
    "\n",
    "@dataclass \n",
    "class NS_Train_Configeration:\n",
    "    artifact_dir:str = os.path.join(ARTIFACTS,TIMESTAMP)\n",
    "    pipeline_dir:str = PIPELINE_DIR\n",
    "    TIMESTAMP:str = TIMESTAMP\n",
    "\n",
    "train_config = NS_Train_Configeration()\n",
    "class Data_validation_config:\n",
    "    data_validation_dir = os.path.join(train_config.artifact_dir,DATA_VALIDATION_DIR)\n",
    "    data_validation_report = os.path.join(data_validation_dir,DATA_VALIDATION_REPORT_DIR,DATA_VALIDATION_REPORT_YAML)\n",
    "\n",
    "# artifact.py\n",
    "@dataclass \n",
    "class Data_validation_Artifact:\n",
    "    validation_status:bool \n",
    "    message_error:str \n",
    "    drift_report_file_path:str\n",
    "\n",
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.constant import SEHEMA_FILE_PATH\n",
    "from Network_Security.utils import read_yaml_file, write_yaml_file\n",
    "from Network_Security.entity.artifact import Data_Ingestion_Artifact, Data_validation_Artifact\n",
    "from Network_Security.entity.config import Data_validation_config\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "from evidently import Report\n",
    "from evidently.presets import DataDriftPreset\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# data_validation.py\n",
    "class Data_validation:\n",
    "    def __init__(self, data_ingestion_artifact: Data_Ingestion_Artifact,\n",
    "                data_validation_config: Data_validation_config):\n",
    "        try:\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_validation_config = data_validation_config\n",
    "            self._schema_yaml = read_yaml_file(file_path=SEHEMA_FILE_PATH)\n",
    "            if self._schema_yaml is None:\n",
    "                raise ValueError(f\"Schema file not loaded or is empty: {SEHEMA_FILE_PATH}\")\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "        \n",
    "    #if number of columns matches schema:\n",
    "    def valid_no_columns(self, dataframe: pd.DataFrame) -> bool:\n",
    "        try:\n",
    "            expected_columns = self._schema_yaml['columns']\n",
    "            status = len(dataframe.columns) == len(expected_columns)\n",
    "            return status\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "\n",
    "    #if all expected columns exist:\n",
    "    def is_column_exists(self, dataframe: pd.DataFrame) -> bool:\n",
    "        try:\n",
    "            missing_num_columns = [col for col in self._schema_yaml['numeric_columns'] if col not in dataframe.columns]\n",
    "            missing_cat_columns = [col for col in self._schema_yaml['categorical_columns'] if col not in dataframe.columns]\n",
    "\n",
    "            if missing_num_columns:\n",
    "                logging.info(f'Missing numeric columns: {missing_num_columns}')\n",
    "            if missing_cat_columns:\n",
    "                logging.info(f'Missing categorical columns: {missing_cat_columns}')\n",
    "\n",
    "            status = not (len(missing_num_columns) > 0 or len(missing_cat_columns) > 0)\n",
    "            return status\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "\n",
    "    def detect_dataset_drift(self, reference_df: pd.DataFrame, current_df: pd.DataFrame) -> bool:\n",
    "        try:\n",
    "            report = Report([DataDriftPreset()],include_tests=\"True\")\n",
    "            report = report.run(reference_data=reference_df, current_data=current_df)\n",
    "            report.save_html(\"data_drift_report.html\")\n",
    "            json_report = report.json()\n",
    "            report_dict = json.loads(json_report)\n",
    "            write_yaml_file(\n",
    "                file_path=self.data_validation_config.data_validation_report,\n",
    "                content=report_dict)\n",
    "            \n",
    "            n_features = sum(1 for m in report_dict[\"metrics\"] if \"ValueDrift\" in m[\"metric_id\"])\n",
    "            drift_metric = next(m for m in report_dict[\"metrics\"] if \"DriftedColumnsCount\" in m[\"metric_id\"])\n",
    "            n_drifted_features = drift_metric[\"value\"][\"count\"]\n",
    "            # Dataset drift status\n",
    "            drift_status = n_drifted_features > 0\n",
    "            print(n_features, n_drifted_features, drift_status)\n",
    "            logging.info(f\"{n_drifted_features}/{n_features} features show drift.\")\n",
    "            return drift_status    \n",
    "        except Exception as e:\n",
    "            logging.info(f\"Error in dataset drift detection: {e}\")\n",
    "            raise NetworkSecurityException (e,sys)\n",
    "  \n",
    "    # Static method to read CSV\n",
    "    @staticmethod\n",
    "    def read_data(file_path: str) -> pd.DataFrame:\n",
    "        return pd.read_csv(file_path)\n",
    "    \n",
    "    def init_data_validation(self) -> Data_validation_Artifact:\n",
    "        try:\n",
    "            valid_message_error = []\n",
    "            # Read train and test data\n",
    "            train_data = self.read_data(self.data_ingestion_artifact.train_file_path)\n",
    "            test_data = self.read_data(self.data_ingestion_artifact.test_file_path)\n",
    "            # train data\n",
    "            if not self.valid_no_columns(train_data):\n",
    "                valid_message_error.append('Error: Column Mismatch in train data')\n",
    "            if not self.is_column_exists(train_data):\n",
    "                valid_message_error.append('Error: Missing columns in train data')\n",
    "            #test data\n",
    "            if not self.valid_no_columns(test_data):\n",
    "                valid_message_error.append('Error: Column Mismatch in test data')\n",
    "            if not self.is_column_exists(test_data):\n",
    "                valid_message_error.append('Error: Missing columns in test data')\n",
    "\n",
    "            # Drift detection\n",
    "            validation_status = len(valid_message_error) == 0\n",
    "            if validation_status:\n",
    "                drift_status = self.detect_dataset_drift(train_data, test_data)\n",
    "                if drift_status:\n",
    "                    valid_message_error.append('Drift detected')\n",
    "                else:\n",
    "                    valid_message_error.append('Drift not detected')\n",
    "            else:\n",
    "                logging.info(f'Validation errors: {valid_message_error}')\n",
    "\n",
    "            #Create artifact\n",
    "            data_validation_artifact = Data_validation_Artifact(\n",
    "                validation_status=validation_status,\n",
    "                message_error=valid_message_error,\n",
    "                drift_report_file_path=self.data_validation_config.data_validation_report\n",
    "            )\n",
    "            return data_validation_artifact\n",
    "\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "\n",
    "# train_pipeline.py \n",
    "from Network_Security.components.data_ingestion import Data_Ingestion\n",
    "from Network_Security.components.data_validation import Data_validation\n",
    "from Network_Security.entity.config import (Data_ingestion_Config,\n",
    "                                            Data_validation_config) \n",
    "from Network_Security.entity.artifact import (Data_Ingestion_Artifact,\n",
    "                                              Data_validation_Artifact)\n",
    "class Training_Pipeline:\n",
    "    def __init__(self):\n",
    "        self.data_ingestion_config = Data_ingestion_Config()\n",
    "        self.data_validation_config = Data_validation_config()\n",
    "\n",
    "\n",
    "    def start_data_ingestion(self)->Data_Ingestion_Artifact:\n",
    "        data_ingestion = Data_Ingestion(ingestion_config=self.data_ingestion_config)\n",
    "        data_ingestion_artifact = data_ingestion.init_data_ingestion()\n",
    "        return data_ingestion_artifact \n",
    "    \n",
    "    def start_data_validation(self, data_ingestion_artifact: Data_Ingestion_Artifact) -> Data_validation_Artifact:\n",
    "        data_valid = Data_validation(data_ingestion_artifact=data_ingestion_artifact,\n",
    "                                    data_validation_config=self.data_validation_config)\n",
    "        data_validation_artifact = data_valid.init_data_validation()\n",
    "        return data_validation_artifact\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run_pipeline(self)->None:\n",
    "        data_ingestion_artifact = self.start_data_ingestion()\n",
    "        data_validation_artifact=self.start_data_validation(data_ingestion_artifact)\n",
    "\n",
    "        return None\n",
    "\n",
    "# app.py \n",
    "from Network_Security.pipeline.train_pipeline import Training_Pipeline\n",
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "import sys \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logging.info('Starting Training Pipeline...')\n",
    "        pipeline = Training_Pipeline()\n",
    "\n",
    "        # Data Ingestion\n",
    "        logging.info('>>> Starting Data Ingestion')\n",
    "        data_ingestion_artifact = pipeline.start_data_ingestion()\n",
    "        logging.info(f'>>> Data Ingestion Completed: {data_ingestion_artifact}')\n",
    "\n",
    "        # Data Validation\n",
    "        logging.info('>>> Starting Data Validation')\n",
    "        data_validation_artifact = pipeline.start_data_validation(data_ingestion_artifact)\n",
    "        logging.info(f'>>> Data Validation Completed: {data_validation_artifact}')\n",
    "\n",
    "        logging.info('Pipeline finished successfully')\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise NetworkSecurityException(e, sys)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e388d",
   "metadata": {},
   "source": [
    "# Data_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e39b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "from datetime import datetime\n",
    "from Network_Security.constant import *\n",
    "TIMESTAMP = datetime.now().strftime('%m_%d_%Y_%H_%M_%S')\n",
    "\n",
    "@dataclass \n",
    "class NS_Train_Configeration:\n",
    "    artifact_dir:str = os.path.join(ARTIFACTS,TIMESTAMP)\n",
    "    pipeline_dir:str = PIPELINE_DIR\n",
    "    TIMESTAMP:str = TIMESTAMP\n",
    "\n",
    "train_config = NS_Train_Configeration()\n",
    "\n",
    "@dataclass \n",
    "class Data_Transformation_Config:\n",
    "    data_transformation_dir = os.path.join(train_config.artifact_dir,DATA_TRANSFORMATION_DIR)\n",
    "    data_transformation_train_file = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_TRANSFORM_FILE,TRAIN_DATA.replace('csv','npy'))\n",
    "    data_transformation_test_file = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_TRANSFORM_FILE,TEST_DATA.replace('csv','npy'))\n",
    "    data_transformation_object_pkl = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_TRANSFORM_0BJECT_FILE,PREPROCESSING_FILE)\n",
    "\n",
    "@dataclass \n",
    "class Data_Transformation_Artifact:\n",
    "    transform_object:str\n",
    "    transform_train_file:str \n",
    "    transform_test_file:str \n",
    "\n",
    "\n",
    "from Network_Security.constant import * \n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "from Network_Security.utils import read_yaml_file, save_object, save_numpy_array\n",
    "from Network_Security.entity.artifact import (\n",
    "    Data_Ingestion_Artifact,\n",
    "    Data_validation_Artifact,\n",
    "    Data_Transformation_Artifact\n",
    ")\n",
    "from Network_Security.entity.config import Data_Transformation_Config\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self,\n",
    "                 data_ingestion_artifact: Data_Ingestion_Artifact,\n",
    "                 data_validation_artifact: Data_validation_Artifact,\n",
    "                 data_transformation_config: Data_Transformation_Config):\n",
    "        try:\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_validation_artifact = data_validation_artifact\n",
    "            self.data_transformation_config = data_transformation_config\n",
    "            self._schema_config = read_yaml_file(SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "    \n",
    "    def get_data_transformation(self) -> Pipeline:\n",
    "        try:\n",
    "            imputer = KNNImputer(**DATA_TRANSFORMATION_IMPUTER_PARAMS)\n",
    "            processor = Pipeline([('imputer', imputer)])\n",
    "            return processor\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_data(file_path: str) -> pd.DataFrame:\n",
    "        try:\n",
    "            return pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "    \n",
    "    def init_data_transformation(self):\n",
    "        try:\n",
    "            train_df = DataTransformation.read_data(self.data_ingestion_artifact.train_file_path)\n",
    "            test_df = DataTransformation.read_data(self.data_ingestion_artifact.test_file_path)\n",
    "\n",
    "            # Train features & target\n",
    "            input_feature_train = train_df.drop(columns=[TARGET_COLUMN], axis=1)\n",
    "            target_feature_train = train_df[TARGET_COLUMN].replace(-1, 0)\n",
    "\n",
    "            # Test features & target\n",
    "            input_feature_test = test_df.drop(columns=[TARGET_COLUMN], axis=1)\n",
    "            target_feature_test = test_df[TARGET_COLUMN].replace(-1, 0)\n",
    "\n",
    "            # Preprocessor\n",
    "            preprocessor = self.get_data_transformation()\n",
    "            input_feature_train_arr = preprocessor.fit_transform(input_feature_train)\n",
    "            input_feature_test_arr = preprocessor.transform(input_feature_test)\n",
    "\n",
    "            # Combine arrays\n",
    "            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train)]\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test)]\n",
    "\n",
    "            # Save transformation pipeline and arrays\n",
    "            save_object(self.data_transformation_config.data_transformation_object_pkl, obj=preprocessor)\n",
    "            save_numpy_array(self.data_transformation_config.data_transformation_train_file, array=train_arr)\n",
    "            save_numpy_array(self.data_transformation_config.data_transformation_test_file, array=test_arr)\n",
    "\n",
    "            data_transformation_artifact = Data_Transformation_Artifact(\n",
    "                transform_object=self.data_transformation_config.data_transformation_object_pkl,\n",
    "                transform_train_file=self.data_transformation_config.data_transformation_train_file,\n",
    "                transform_test_file=self.data_transformation_config.data_transformation_test_file\n",
    "            )\n",
    "\n",
    "            return data_transformation_artifact\n",
    "\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e06811",
   "metadata": {},
   "source": [
    "# Practice Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "def read_yaml_file(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'rb') as file:\n",
    "        yaml.safe_load(file) \n",
    "\n",
    "def read_data(df:pd.DataFrame):\n",
    "    return pd.read_csv(df)\n",
    "\n",
    "def drop_col(df:pd.DataFrame,col):\n",
    "    return df.drop(columns=[col],axis=1,inplace=True)\n",
    "\n",
    "class TargetValueMapping:\n",
    "    def __init__(self):\n",
    "        self.male = 0\n",
    "        self.female = 1 \n",
    "    def _asdict(self):\n",
    "        return self.__dict__ \n",
    "    def reverse(self):\n",
    "        return dict(zip(self._asdict().values(), self._asdict().keys()))\n",
    "    \n",
    "def save_numpy_array(file_path: str, array: np.array):\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(file_path, 'wb') as file_obj:\n",
    "            np.save(file_obj, array)\n",
    "\n",
    "def save_object(file_path: str,obj):\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(file_path, 'wb') as file_obj:\n",
    "            pickle.dump(file_obj,obj)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "import os \n",
    "from datetime import datetime \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Data_Ingestion_Artifact:\n",
    "    train_file_path:str \n",
    "    test_file_path:str\n",
    "\n",
    "@dataclass\n",
    "class Data_validation_Artifact:\n",
    "    validation_status:bool\n",
    "    message: str\n",
    "    drift_report_file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationArtifact:\n",
    "    transformed_object_file_path:str \n",
    "    transformed_train_file_path:str\n",
    "    transformed_test_file_path:str\n",
    "\n",
    "# Artifacts\n",
    "ARTIFACTS = 'artifacts'\n",
    "PIPELINE_DIR = 'network'\n",
    "@dataclass \n",
    "class NS_Train_Configeration:\n",
    "    artifact_dir:str = ARTIFACTS\n",
    "    pipeline_dir:str = PIPELINE_DIR\n",
    "    TIMESTAMP = datetime.now().strftime('%m_%d_%Y_%H_%M_%S')\n",
    "\n",
    "train_config = NS_Train_Configeration() \n",
    "\n",
    "DATA_TRANSFORMATION_DIR:str = 'data_transformation'\n",
    "DATA_TRANSFORMATION_TRANSFORM_FILE:str = 'transform'\n",
    "DATA_TRANSFORMATION_OBJECT_DIR:str = 'transform_object' \n",
    "\n",
    "# data\n",
    "RAW_DATA = 'raw.csv'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "TEST_DATA = 'test.csv'\n",
    "PREPROCESSOR_FILE = 'preprocessor.pkl'\n",
    "TARGET_COLUMN = '------------------------------------------'\n",
    "CURRENT_DATE = datetime.now()\n",
    "\n",
    "class Data_Transformation_config:\n",
    "    data_transformation_dir = os.path.join(train_config.artifact_dir,DATA_TRANSFORMATION_DIR)\n",
    "    data_transformation_train_file = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_TRANSFORM_FILE,TRAIN_DATA.replace('csv','npy'))\n",
    "    data_transformation_test_file = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_TRANSFORM_FILE,TEST_DATA.replace('csv','npy'))\n",
    "    data_transformation_object = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_OBJECT_DIR,PREPROCESSOR_FILE)\n",
    "\n",
    "from Network_Security.constant import SCHEMA_FILE_PATH\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler,PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "@dataclass\n",
    "class DataTransformationArtifact:\n",
    "    transformed_object_file_path:str \n",
    "    transformed_train_file_path:str\n",
    "    transformed_test_file_path:str\n",
    "\n",
    "class Data_Transformation:\n",
    "    def __init__(self,data_ingestion_artifact=Data_Ingestion_Artifact,\n",
    "                 data_validation_artifact=Data_validation_Artifact,\n",
    "                 data_transformation_config=Data_Transformation_config):\n",
    "        self.data_ingestion_artifact = data_ingestion_artifact\n",
    "        self.data_validation_artifact= data_validation_artifact\n",
    "        self.data_transformation_config= data_transformation_config\n",
    "        self._sehema = read_yaml_file(file_path=SCHEMA_FILE_PATH)\n",
    "    def get_data_transformation(self):\n",
    "        ohe_transform = OneHotEncoder()\n",
    "        or_transform = OrdinalEncoder()\n",
    "        scaler = StandardScaler()\n",
    "        pw_transform = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "        ohe_col = self._sehema['ohe_columns']\n",
    "        or_col = self._sehema['or_columns']\n",
    "        num_col = self._sehema['numerical_columns']\n",
    "        power_tf_col = self._sehema['transform_columns']\n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('OneHotEncoder',ohe_transform,ohe_col),\n",
    "            ('OrdinalEncoder',or_transform,or_col),\n",
    "            ('PowerTransformer',pw_transform,power_tf_col),\n",
    "            ('StandardScaler',scaler,num_col)\n",
    "\n",
    "        ])\n",
    "        return preprocessor \n",
    "    @staticmethod\n",
    "    def read_data(dataframe:pd.DataFrame):\n",
    "        return pd.read_csv(dataframe)\n",
    "\n",
    "    def init_data_transformation(self):\n",
    "        if self.data_validation_artifact.validation_status:\n",
    "            preprocessor = self.get_data_transformation()\n",
    "            train_df = Data_Transformation.read_data(self.data_ingestion_artifact.train_file_path)\n",
    "            test_df = Data_Transformation.read_data(self.data_ingestion_artifact.test_file_path)\n",
    "            \n",
    "            xtrain = train_df.drop(columns=[TARGET_COLUMN],axis=1)\n",
    "            ytrain = train_df[TARGET_COLUMN]\n",
    "\n",
    "            xtrain['company_age'] = CURRENT_DATE - xtrain['company_estabilish']\n",
    "            drop_col = self._sehema['drop_col']\n",
    "            xtrain=drop_col(xtrain,drop_col)\n",
    "\n",
    "            ytrain = ytrain.replace(TargetValueMapping()._asdict())\n",
    "\n",
    "            #test-df\n",
    "            xtest = test_df.drop(columns=[TARGET_COLUMN],axis=1)\n",
    "            ytest = test_df[TARGET_COLUMN]\n",
    "\n",
    "            xtest['company_age'] = CURRENT_DATE - xtest['company_estabilish']\n",
    "            drop_col = self._sehema['drop_col']\n",
    "            xtest=drop_col(xtest,drop_col)\n",
    "\n",
    "            ytest = ytest.replace(TargetValueMapping()._asdict())\n",
    "\n",
    "            xtrain_arr = preprocessor.fit_transform(xtrain)\n",
    "            xtest_arr = preprocessor.transform(xtest)\n",
    "\n",
    "            from imblearn.combine import SMOTEENN \n",
    "            smt = SMOTEENN(sampling_strategy=\"minority\")\n",
    "            xtrain_arr,ytrain = smt.fit_resample(xtrain_arr,ytrain)\n",
    "\n",
    "            smt = SMOTEENN(sampling_strategy=\"minority\")\n",
    "            xtest_arr,ytest = smt.fit_resample(xtest_arr,ytest)\n",
    "\n",
    "            train_arr = np.c_[xtrain_arr,np.array(ytrain)]\n",
    "            test_arr = np.c_[xtest_arr,np.array(ytest)]\n",
    "\n",
    "            save_object(self.data_transformation_config.data_transformation_object,preprocessor)\n",
    "            save_numpy_array(self.data_transformation_config.data_transformation_train_file,array=train_arr)\n",
    "            save_numpy_array(self.data_transformation_config.data_transformation_train_file,array=test_arr)\n",
    "\n",
    "            return DataTransformationArtifact(transformed_object_file_path= self.data_transformation_config.transformed_object_file_path,\n",
    "                                                transformed_train_file_path= self.data_transformation_config.transformed_train_file_path,\n",
    "                                                transformed_test_file_path= self.data_transformation_config.transformed_test_file_path\n",
    "                                                )\n",
    "                \n",
    "\n",
    "class Training_Pipeline:\n",
    "    def __init__(self):\n",
    "    #   self.data_ingestion_config = Data_ingestion_Config()\n",
    "    #   self.validation_config = Data_validation_config()\n",
    "        self.data_transformation_config= Data_Transformation_config()\n",
    "\n",
    "\n",
    "\n",
    "    # def start_data_ingestion(self)->Data_Ingestion_Artifact:\n",
    "    #     data_ingestion = Data_Ingestion(ingestion_config=self.data_ingestion_config)\n",
    "    #     data_ingestion_artifacet = data_ingestion.init_data_ingestion()\n",
    "    #     return data_ingestion_artifacet\n",
    "\n",
    "    # def start_data_validation(self,data_ingestion_artifacet:Data_Ingestion_Artifact)-> Data_validation_Artifact:\n",
    "    #     data_valid = Data_validation(data_ingestion_artifacet=data_ingestion_artifacet,\n",
    "    #                                   data_validation_config=self.data_validation_config)\n",
    "    #     data_validation_Artifact = data_valid.init_data_ingestion()\n",
    "    #     return data_validation_Artifact\n",
    "\n",
    "    def start_data_transform(self,data_ingestion_artifacet:Data_Ingestion_Artifact,\n",
    "                             data_validation_Artifact:Data_validation_Artifact)->DataTransformationArtifact:\n",
    "        data_transform_config = Data_Transformation(\n",
    "                                                    data_ingestion_artifacet=data_ingestion_artifacet,\n",
    "                                                    data_validation_Artifact=data_validation_Artifact,\n",
    "                                                    data_transformation_config=self.data_transformation_config\n",
    "                                                    )\n",
    "        data_transform_artifact = data_transform_config.init_data_transformation()\n",
    "        return data_transform_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b276c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10073ff6",
   "metadata": {},
   "source": [
    "# Model_Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07bc633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MODEL_TRAINER_DIR: str = 'model_trainer'\n",
    "MODEL_TRAINER_FILE_NAME: str = 'trained_model'\n",
    "MODEL_TRAINER_TRAINED_MODEL_NAME: str = 'model.pkl'\n",
    "MODEL_TRAINER_CONFIG_PARAM_PATH: str = os.path.join('data_schema', 'best_param.yaml')\n",
    "MODEL_TRAINER_EXCEPTED_RATIO: float = 0.6\n",
    "\n",
    "# config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass \n",
    "class Model_Trainer_Config:\n",
    "    model_trainer_dir = os.path.join(train_config.artifact_dir, MODEL_TRAINER_DIR)\n",
    "    model_trained_path = os.path.join(model_trainer_dir, MODEL_TRAINER_FILE_NAME, MODEL_TRAINER_TRAINED_MODEL_NAME)\n",
    "    model_trained_config_param_path = MODEL_TRAINER_CONFIG_PARAM_PATH\n",
    "    excepted_ratio = MODEL_TRAINER_EXCEPTED_RATIO\n",
    "    metrics_artifact = Metrics_Artifact\n",
    "\n",
    "# artifact\n",
    "@dataclass \n",
    "class Metrics_Artifact:\n",
    "    f1_score: float\n",
    "    accuracy_score: float\n",
    "    recall_score: float\n",
    "    precision_score: float\n",
    "\n",
    "@dataclass \n",
    "class Model_Trainer_Artifact:\n",
    "    model_pkl: str\n",
    "    metrics_artifact: Metrics_Artifact\n",
    "\n",
    "\n",
    "# model_trainer\n",
    "from Network_Security.entity.artifact import (Data_Transformation_Artifact,\n",
    "                                              Metrics_Artifact,\n",
    "                                              Model_Trainer_Arifact)\n",
    "from Network_Security.entity.config import Model_Trainer_Config\n",
    "from Network_Security.utils import load_numpy_array,load_object,save_object\n",
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from neuro_mf import ModelFactory\n",
    "from typing import Tuple\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "class Network_model:\n",
    "    def __init__(self, transform_object: Pipeline, best_model_details: object)->Tuple[object,object]:\n",
    "        self.transform_object = transform_object\n",
    "        self.best_model_details = best_model_details\n",
    "    def predict(self, dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            transformed_features = self.transform_object.transform(dataframe)\n",
    "            predictions = self.best_model_details.predict(transformed_features)\n",
    "\n",
    "            return pd.DataFrame(predictions, columns=['prediction'])\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "\n",
    "class Model_Train:\n",
    "    def __init__(self, data_transformation_artifact: Data_Transformation_Artifact,\n",
    "                 model_trainer_config: Model_Trainer_Config):\n",
    "        self.data_transformation_artifact = data_transformation_artifact\n",
    "        self.model_trainer_config = model_trainer_config\n",
    "    \n",
    "    def get_best_model_indentify(self, train_arr: np.array, test_arr: np.array):\n",
    "        try:\n",
    "            model_factory = ModelFactory(self.model_trainer_config.model_trained_config_param_path)\n",
    "        \n",
    "            xtrain, ytrain = train_arr[:, :-1], train_arr[:, -1]\n",
    "            xtest, ytest = test_arr[:, :-1], test_arr[:, -1]\n",
    "\n",
    "            best_model_details = model_factory.get_best_model(\n",
    "            X=xtrain,\n",
    "            y=ytrain,\n",
    "            base_accuracy=self.model_trainer_config.excepted_ratio)\n",
    "            \n",
    "            best_model = best_model_details.best_model\n",
    "            print(best_model)\n",
    "            pred = best_model.predict(xtest)\n",
    "\n",
    "            acc = accuracy_score(ytest, pred)\n",
    "            f1 = f1_score(ytest, pred)\n",
    "            recall = recall_score(ytest, pred)\n",
    "            precision = precision_score(ytest, pred)\n",
    "            \n",
    "            metrics_artifact = Metrics_Artifact(f1_score=f1, accuracy_score=acc, recall_score=recall, precision_score=precision)\n",
    "            print(metrics_artifact)\n",
    "            print(best_model_details.best_score)\n",
    "            print(best_model_details.best_parameters)\n",
    "            \n",
    "            return best_model_details, metrics_artifact\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "    \n",
    "    def init_best_model(self):\n",
    "        try:\n",
    "            train_arr = load_numpy_array(self.data_transformation_artifact.transform_train_file)\n",
    "            test_arr = load_numpy_array(self.data_transformation_artifact.transform_test_file)\n",
    "\n",
    "            best_model_details, metrics_artifact = self.get_best_model_indentify(train_arr, test_arr)\n",
    "            transform_object = load_object(self.data_transformation_artifact.transform_object)\n",
    "        \n",
    "            if best_model_details.best_score < self.model_trainer_config.excepted_ratio:\n",
    "                logging.info(\"Best model not found with expected accuracy.\")\n",
    "\n",
    "            network_model_obj = Network_model(transform_object, best_model_details)\n",
    "            save_object(self.model_trainer_config.model_trained_path, network_model_obj)\n",
    "\n",
    "            model_trainer_artifact = Model_Trainer_Arifact(\n",
    "                model_pkl=self.model_trainer_config.model_trained_path,\n",
    "                metrics=metrics_artifact\n",
    "            )\n",
    "\n",
    "            return model_trainer_artifact\n",
    "        except Exception as e:\n",
    "                raise NetworkSecurityException(e,sys)\n",
    "\n",
    "\n",
    "def strat_model_trainer(self,data_transformation_artifact:Data_Transformation_Artifact):\n",
    "        try:\n",
    "            model_train = Model_Train(data_transformation_artifact=data_transformation_artifact,\n",
    "                                    model_trainer_config=self.model_trainer_config)\n",
    "            model_trainer_artifact=model_train.init_best_model()\n",
    "            return model_trainer_artifact\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "\n",
    "def run_pipeline(self)->None:\n",
    "    try:\n",
    "        data_ingestion_artifact = self.start_data_ingestion()\n",
    "        data_validation_artifact=self.start_data_validation(data_ingestion_artifact)\n",
    "        data_transformation_artifact = self.start_data_transformation(data_ingestion_artifact,data_validation_artifact)\n",
    "        model_trainer_artifact = self.strat_model_trainer(data_transformation_artifact)\n",
    "        return model_trainer_artifact\n",
    "    except Exception as e:\n",
    "        raise NetworkSecurityException(e,sys)\n",
    "\n",
    "   \n",
    "\n",
    "from Network_Security.pipeline.train_pipeline import Training_Pipeline\n",
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "import sys \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logging.info('Starting Training Pipeline...')\n",
    "        pipeline = Training_Pipeline()\n",
    "\n",
    "        # Data Ingestion\n",
    "        logging.info('>>> Starting Data Ingestion')\n",
    "        data_ingestion_artifact = pipeline.start_data_ingestion()\n",
    "        logging.info(f'>>> Data Ingestion Completed: {data_ingestion_artifact}')\n",
    "\n",
    "        # Data Validation\n",
    "        logging.info('---------->>> Starting Data Validation-------------->>>')\n",
    "        data_validation_artifact = pipeline.start_data_validation(data_ingestion_artifact)\n",
    "        logging.info(f'>>> Data Validation Completed: {data_validation_artifact}')\n",
    "\n",
    "        # Data Transformation\n",
    "        logging.info('>>> Starting Data Transformation')\n",
    "        data_transformation_artifact = pipeline.start_data_transformation(data_ingestion_artifact,data_validation_artifact)\n",
    "        logging.info(f'>>> Data Transformation Completed: {data_transformation_artifact}')\n",
    "\n",
    "        #Model Trainer\n",
    "        logging.info('---------->>> Starting Model Trainer -------------->>>')\n",
    "        model_trainer_artifact = pipeline.strat_model_trainer(data_transformation_artifact)\n",
    "        logging.info(f'>>> Model Trainer Completed: {model_trainer_artifact}')\n",
    "\n",
    "\n",
    "        logging.info('Pipeline finished successfully')\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise NetworkSecurityException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0992ff",
   "metadata": {},
   "source": [
    "# Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6454d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS Configeration\n",
    "REGION = 'us-east-1'\n",
    "AWS_ACCESS_KEY = 'AWS_ACCESS_KEY_ID'\n",
    "AWS_SECRET_KEY = 'AWS_SECRET_ACCESS_KEY' \n",
    "\n",
    "# model_evalution \n",
    "MODEL_BUCKET_NAME:str = 'network_security'\n",
    "MODEL_EVALUTION_CHANGED_THRESHOLD:float = 0.8 \n",
    "MODEL_TRAINER_TRAINED_MODEL_NAME:str = 'model.pkl'\n",
    "\n",
    "@dataclass \n",
    "class Model_Evalution_Config:\n",
    "    bucket_name:str = MODEL_BUCKET_NAME \n",
    "    s3_model_path:str = MODEL_TRAINER_TRAINED_MODEL_NAME\n",
    "    changed_model_score:float =  MODEL_EVALUTION_CHANGED_THRESHOLD\n",
    "\n",
    "@dataclass \n",
    "class Model_Evalution_Artifact:\n",
    "    is_model_accepted:bool \n",
    "    changed_accuracy:float\n",
    "    train_model_path:str \n",
    "    s3_model_path:str\n",
    "\n",
    "\n",
    "# configeration/aws_conection.py\n",
    "import os\n",
    "import boto3\n",
    "from Network_Security.constant import (REGION,AWS_ACCESS_KEY,AWS_SECRET_KEY)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class S3Client:\n",
    "    s3_client = None\n",
    "    s3_resource = None \n",
    "\n",
    "    def __init__(self, region_name=REGION):    \n",
    "        _access_key = os.getenv(AWS_ACCESS_KEY)\n",
    "        _secret_key = os.getenv(AWS_SECRET_KEY)  \n",
    "\n",
    "        if _access_key is None or _secret_key is None:\n",
    "            raise ValueError(\"Missing AWS credentials in environment variables\")\n",
    "\n",
    "        if S3Client.s3_client is None and S3Client.s3_resource is None:\n",
    "\n",
    "            S3Client.s3_resource = boto3.resource(\n",
    "                's3',\n",
    "                aws_access_key_id=_access_key,\n",
    "                aws_secret_access_key=_secret_key,\n",
    "                region_name=region_name\n",
    "            )\n",
    "            S3Client.s3_client = boto3.client(\n",
    "                's3',\n",
    "                aws_access_key_id=_access_key,\n",
    "                aws_secret_access_key=_secret_key,\n",
    "                region_name=region_name\n",
    "            )\n",
    "        self.s3_client = S3Client.s3_client\n",
    "        self.s3_resource = S3Client.s3_resource\n",
    "\n",
    "\n",
    "#cloud/aws_service.py\n",
    "from Network_Security.configeration.aws_connection import S3Client\n",
    "from typing import List,Union\n",
    "from io import StringIO\n",
    "import pickle\n",
    "class SimpleStorageService:\n",
    "    def __init__(self):\n",
    "        s3_client = S3Client()\n",
    "        self.s3_client = s3_client.s3_client\n",
    "        self.resource = s3_client.resource\n",
    "\n",
    "    def s3_key_path_available(self,bucket_name,s3_key)->bool:\n",
    "        bucket = self.resource.Bucket(bucket_name)\n",
    "        file_object = [file_object for file_object in bucket.objects.filter(prefix=s3_key)]\n",
    "        if len(file_object)>0:\n",
    "            return True \n",
    "        else:\n",
    "            return False  \n",
    "    def get_file_object(self,bucket_name,model_path)->Union[List[object],object]:\n",
    "        bucket = self.resource.Bucket(bucket_name)\n",
    "        file_object = [file_object for file_object in bucket.objects.filter(prefix=model_path)]\n",
    "        func = lambda x:x[0] if len(x)==1 else x\n",
    "        file_obj = func(file_object)\n",
    "        return file_obj\n",
    "    \n",
    "# এই কোডের উদ্দেশ্য —\n",
    "# নির্দিষ্ট S3 bucket থেকে model_path অনুযায়ী ফাইল খুঁজে এনে,\n",
    "# যদি একটাই ফাইল থাকে তাহলে সেটি ফেরত দাও,\n",
    "# আর একাধিক থাকলে পুরো তালিকাটি ফেরত দাও।\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def read_object(file_object,decode: bool=True,model_readable: bool=False):\n",
    "        func = (lambda:file_object.get()['Body'].read().decode()\n",
    "                if decode is True \n",
    "                else file_object.get()['Body'].read())\n",
    "        conv_func = lambda:StringIO(func()) if model_readable is True else func()\n",
    "        return conv_func \n",
    "    \n",
    "    def load_model(self,bucket_name,model_name,model_dir=None):\n",
    "        func = (lambda: model_name\n",
    "                if model_dir is None\n",
    "                else model_dir + '/' + model_name)\n",
    "        model_path = func()\n",
    "        file_object = self.get_file_object(bucket_name=bucket_name,model_path=model_path)\n",
    "        model_object = self.read_object(file_object,decode=False)\n",
    "        model = pickle.load(model_object)\n",
    "        return model\n",
    "\n",
    "# | ফাংশন               | কাজ                                                        |\n",
    "# | ------------------- | ---------------------------------------------------------- |\n",
    "# | `get_file_object()` | S3 থেকে ফাইল অবজেক্ট বের করে আনা                           |\n",
    "# | `read_object()`     | সেই ফাইলের ডেটা পড়া (string বা bytes আকারে)                |\n",
    "# | `load_model()`      | S3 থেকে মডেল (.pkl) ফাইল নিয়ে `pickle.load()` দিয়ে লোড করা |\n",
    "\n",
    "\n",
    "# entity/estimator.py\n",
    "class Network_model:\n",
    "    def __init__(self, transform_object: Pipeline, best_model_details: object):\n",
    "        self.transform_object = transform_object\n",
    "        self.best_model_details = best_model_details\n",
    "\n",
    "    def predict(self, dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            transformed_features = self.transform_object.transform(dataframe)\n",
    "            predictions = self.best_model_details.predict(transformed_features)\n",
    "\n",
    "            return pd.DataFrame(predictions, columns=['prediction'])\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "    def __repr__(self):\n",
    "        return f\"{type(self.best_model_details).__name__}()\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{type(self.best_model_details).__name__}()\"\n",
    "\n",
    "#entity/s3_estimatir.py\n",
    "from Network_Security.entity.estimator import Network_model\n",
    "#from Network_Security.components.model_train import Network_model\n",
    "from Network_Security.cloud.aws_service import SimpleStorageService\n",
    "\n",
    "class NetworkEstimator:\n",
    "    def __init__(self,bucket_name,model_path):\n",
    "        self.model_path = model_path \n",
    "        self.bucket_name = bucket_name\n",
    "        self.s3 = SimpleStorageService() \n",
    "        self.load_model:Network_model=None\n",
    "\n",
    "    def is_model_present(self,model_path):\n",
    "     return self.s3.s3_key_path_available(bucket_name=self.bucket_name,\n",
    "                                          model_path=model_path)\n",
    "    def load_model(self)->Network_model:\n",
    "        model_pkl = self.s3.load_model(bucket_name=self.bucket_name,\n",
    "                                  model_name=self.model_path)\n",
    "        return model_pkl\n",
    "\n",
    "\n",
    "# component/model_evalution.py\n",
    "from Network_Security.entity.artifact import (Data_Ingestion_Artifact,\n",
    "                                              Model_Trainer_Artifact,\n",
    "                                              Model_Evalution_Artifact)\n",
    "from Network_Security.entity.config import Model_Evalution_Config\n",
    "from Network_Security.entity.s3_estimator import NetworkEstimator\n",
    "from Network_Security.constant import TARGET_COLUMN\n",
    "from sklearn.metrics import f1_score\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass \n",
    "class ModelEvalutionResponse:\n",
    "    is_model_accept:bool\n",
    "    difference:float \n",
    "    train_model_f1_score:float \n",
    "    s3_model_f1_score:float \n",
    "\n",
    "class ModelEvalution:\n",
    "    def __init__(self,data_ingestion_artifact:Data_Ingestion_Artifact ,\n",
    "                 model_trainer_artifact:Model_Trainer_Artifact,\n",
    "                 model_evalution_config:Model_Evalution_Config):\n",
    "        self.data_ingestion_artifact = data_ingestion_artifact\n",
    "        self.model_trainer_artifact = model_trainer_artifact\n",
    "        self.model_evalution_config = model_evalution_config\n",
    "\n",
    "    def get_best_model(self):\n",
    "        bucket_name = self.model_evalution_config.bucket_name \n",
    "        model_path = self.model_evalution_config.s3_model_path \n",
    "        network_model = NetworkEstimator(bucket_name,model_path)\n",
    "        if network_model.is_model_present(model_path):\n",
    "            return True\n",
    "            # return network_model.load_model()\n",
    "        return None \n",
    "    def evaluate_model(self):\n",
    "        test_df = pd.read_csv(self.data_ingestion_artifact.test_file_path)\n",
    "        x = test_df.drop([TARGET_COLUMN],axis=1)\n",
    "        y = test_df[TARGET_COLUMN]\n",
    "        y = y.repalce(-1,0) \n",
    "        best_model = self.get_best_model()\n",
    "        best_model_score = None\n",
    "         \n",
    "        if best_model is not None:\n",
    "            pred = best_model.predict(x)\n",
    "            best_model_score = f1_score(y,pred)\n",
    "        temp_model_score = 0 if best_model_score is None else best_model_score\n",
    "        train_model_score = self.model_trainer_artifact.metrics.f1_score\n",
    "\n",
    "        model_evalution_response = ModelEvalutionResponse(\n",
    "            is_model_accept=train_model_score > temp_model_score,\n",
    "            difference=train_model_score - temp_model_score,\n",
    "            train_model_f1_score=train_model_score,\n",
    "            s3_model_f1_score=best_model_score\n",
    "        )\n",
    "        return model_evalution_response\n",
    "    \n",
    "    def init_model_evaluation(self):\n",
    "        model_evalution_response = self.evaluate_model()\n",
    "        s3_model = self.model_evalution_config.s3_model_path \n",
    "        model_evalution_artifact = Model_Evalution_Artifact(\n",
    "            is_model_accepted= model_evalution_response.is_model_accept,\n",
    "            changed_accuracy=model_evalution_response.difference,\n",
    "            s3_model_path=s3_model,\n",
    "            train_model_path=self.model_trainer_artifact.model_pkl\n",
    "        )\n",
    "        return model_evalution_artifact\n",
    "\n",
    "# class Training_Pipeline\n",
    "    def start_model_evalution(self,data_ingestion_artifact:Data_Ingestion_Artifact,\n",
    "                              model_trainer_artifact:Model_Trainer_Artifact)->Model_Evalution_Artifact:\n",
    "            try:\n",
    "                model_evaluate = ModelEvalution(data_ingestion_artifact = data_ingestion_artifact, \n",
    "                                                model_trainer_artifact = model_trainer_artifact, \n",
    "                                                model_evalution_config = self.model_evalution_config)\n",
    "                model_evalution_artifact = model_evaluate.init_model_evaluation()\n",
    "                return model_evalution_artifact\n",
    "            except Exception as e:\n",
    "                raise NetworkSecurityException(e,sys)\n",
    "\n",
    "    def run_pipeline(self)->None:\n",
    "        try:\n",
    "            data_ingestion_artifact = self.start_data_ingestion()\n",
    "            data_validation_artifact=self.start_data_validation(data_ingestion_artifact)\n",
    "            data_transformation_artifact = self.start_data_transformation(data_ingestion_artifact,data_validation_artifact)\n",
    "            model_trainer_artifact = self.strat_model_trainer(data_transformation_artifact)\n",
    "            model_evalution_artifact = self.start_model_evalution(data_ingestion_artifact,model_trainer_artifact)\n",
    "            return model_evalution_artifact\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "\n",
    "\n",
    "# main.py\n",
    "from Network_Security.pipeline.train_pipeline import Training_Pipeline\n",
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "import sys \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logging.info('Starting Training Pipeline...')\n",
    "        pipeline = Training_Pipeline()\n",
    "\n",
    "        # Data Ingestion\n",
    "        logging.info('>>> Starting Data Ingestion')\n",
    "        data_ingestion_artifact = pipeline.start_data_ingestion()\n",
    "        logging.info(f'>>> Data Ingestion Completed: {data_ingestion_artifact}')\n",
    "\n",
    "        # Data Validation\n",
    "        logging.info('---------->>> Starting Data Validation-------------->>>')\n",
    "        data_validation_artifact = pipeline.start_data_validation(data_ingestion_artifact)\n",
    "        logging.info(f'>>> Data Validation Completed: {data_validation_artifact}')\n",
    "\n",
    "        # Data Transformation\n",
    "        logging.info('---------->>> Starting Data Transformation---------->>>')\n",
    "        data_transformation_artifact = pipeline.start_data_transformation(data_ingestion_artifact,data_validation_artifact)\n",
    "        logging.info(f'>>>Data Transformation Completed: {data_transformation_artifact}')\n",
    "\n",
    "        #Model Trainer\n",
    "        logging.info('---------->>> Starting Model Trainer -------------->>>')\n",
    "        model_trainer_artifact = pipeline.strat_model_trainer(data_transformation_artifact)\n",
    "        logging.info(f'>>> Model Trainer Completed: {model_trainer_artifact}')\n",
    "\n",
    "        #Model Evalution\n",
    "        logging.info('---------->>> Starting Model evalution -------------->>>')\n",
    "        model_evalution_artifact = pipeline.start_model_evalution(data_ingestion_artifact,model_trainer_artifact)\n",
    "        logging.info(f'>>> Model Evalution Completed: {model_evalution_artifact}')\n",
    "\n",
    "\n",
    "        logging.info('Pipeline finished successfully')\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise NetworkSecurityException(e, sys)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
