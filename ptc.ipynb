{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34cb1bf5",
   "metadata": {},
   "source": [
    "# DataIngestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b481a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np \n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DATABASE_NAME = 'NETWORK_SECURITY'\n",
    "COLLECTION_NAME = 'NETWORK_DATA' \n",
    "MONGODB_URL = 'MONGODB_URL'\n",
    "\n",
    "ARTIFACTS_DIR = 'artifacts'\n",
    "PIPELINE_DIR = 'security'\n",
    "\n",
    "DATA_INGESTION_DIR_NAME : str = 'data_ingestion'\n",
    "DATA_INGESTION_COLLECTION_NAME: str = 'NETWORK_DATA'\n",
    "DATA_INGESTION_FEATURE_STORED_NAME:str = 'feature'\n",
    "DATA_INGESTION_INGESTED_NAME:str = 'ingested'\n",
    "DATA_INGESTION_SPLIT_RATIO:float = 0.2 \n",
    "\n",
    "RAW_DATA = 'security.csv'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "TEST_DATA = 'test.csv'\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime('%m_%d_%Y_%H_%M_%S')\n",
    "\n",
    "@dataclass \n",
    "class TrainingConfiguration:\n",
    "    artifact_dir:str = ARTIFACTS_DIR \n",
    "    piprline_dir:str = PIPELINE_DIR \n",
    "    timestamp:str = TIMESTAMP \n",
    "\n",
    "trainingconfig : TrainingConfiguration=TrainingConfiguration()\n",
    "\n",
    "@dataclass \n",
    "class Data_ingestion_configeration:\n",
    "    data_ingestion_dir:str = os.path.join(trainingconfig.artifact_dir,DATA_INGESTION_DIR_NAME)\n",
    "    data_ingestion_collection:str = DATA_INGESTION_COLLECTION_NAME \n",
    "    data_ingestion_feature:str = os.path.join(data_ingestion_dir,DATA_INGESTION_FEATURE_STORED_NAME,RAW_DATA)\n",
    "    train_data_path:str = os.path.join(data_ingestion_dir,DATA_INGESTION_INGESTED_NAME,TRAIN_DATA)\n",
    "    test_data_path:str = os.path.join(data_ingestion_dir,DATA_INGESTION_INGESTED_NAME,TEST_DATA)\n",
    "    split_ratio:float = DATA_INGESTION_SPLIT_RATIO\n",
    "\n",
    "\n",
    "\n",
    "# mongodb\n",
    "from Network_Security.constant import MONGOBD_URL, DATA_BASE_NAME\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "from Network_Security.logging.logger import logging\n",
    "from dotenv import load_dotenv \n",
    "import certifi \n",
    "import pymongo\n",
    "import sys\n",
    "import os \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "# MONGOBD_URL = os.getenv(\"MONGOBD_URL\")\n",
    "ca = certifi.where()  \n",
    "\n",
    "class MongoDBClient:\n",
    "    def __init__(self, database=DATA_BASE_NAME):\n",
    "        try:\n",
    "            mongo_url = os.getenv(MONGOBD_URL)\n",
    "            if mongo_url is None:\n",
    "                logging.info(\"MongoDB URL not found in environment variables\")\n",
    "                raise ValueError(\"MongoDB URL is missing\")\n",
    "\n",
    "            MongoDBClient.client = pymongo.MongoClient(mongo_url, tlsCAFile=ca)\n",
    "            self.client = MongoDBClient.client \n",
    "            self.database = self.client[database]\n",
    "            self.database_name = database  \n",
    "\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    " \n",
    "\n",
    "#networkdata_acces\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.configeration.mongodb import MongoDBClient \n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sys\n",
    "\n",
    "\n",
    "class NetworkData:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.mongo_client = MongoDBClient()   \n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "\n",
    "    def get_dataframe(self, collection_name: str, database_name: Optional[str] = None)->pd.DataFrame:\n",
    "        try:\n",
    "            if database_name:\n",
    "                collection = self.mongo_client.client[database_name][collection_name]\n",
    "            else:\n",
    "                collection = self.mongo_client.database[collection_name]\n",
    "\n",
    "            df = pd.DataFrame(list(collection.find()))\n",
    "            if \"_id\" in df.columns:\n",
    "                df.drop(columns=[\"_id\"], inplace=True)\n",
    "            df.replace(\"na\", np.nan, inplace=True)\n",
    "\n",
    "            logging.info(\"DataFrame Extract Successful\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "\n",
    "\n",
    "# @dataclass \n",
    "# class TrainingPipelineConfig:\n",
    "#     TRAIN_DATA_PATH:str \n",
    "#     TEST_DATA:str \n",
    "\n",
    "# data_ingestion\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.entity.config import Data_ingestion_Config\n",
    "from Network_Security.entity.artifact import Data_Ingestion_Artifact\n",
    "from Network_Security.configeration.mongodb import MongoDBClient  \n",
    "from Network_Security.data_acess.networkdata_acess import NetworkData \n",
    "\n",
    "class Data_Ingestion:\n",
    "    def __init__(self, ingestion_config: Data_ingestion_Config):\n",
    "        try:\n",
    "            self.ingestion_config = ingestion_config\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "\n",
    "    def get_feature_extract_data(self):\n",
    "        try:\n",
    "            logging.info(\"Extracting data from MongoDB...\")\n",
    "            networkdata = NetworkData()\n",
    "            \n",
    "            dataframe = networkdata.get_dataframe(\n",
    "                collection_name=self.ingestion_config.data_ingestion_collection_path\n",
    "            )\n",
    "            # start feature_store\n",
    "            feature_data_path = self.ingestion_config.data_ingestion_feature_path\n",
    "            os.makedirs(os.path.dirname(feature_data_path), exist_ok=True)\n",
    "            dataframe.to_csv(feature_data_path, index=False, header=True)\n",
    "            logging.info(f\"Data stored at {feature_data_path}\")\n",
    "            return dataframe\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "\n",
    "    def split_data(self, dataframe: pd.DataFrame):\n",
    "        try:\n",
    "            train_data, test_data = train_test_split(\n",
    "                dataframe, \n",
    "                test_size=self.ingestion_config.split_ratio\n",
    "            )\n",
    "\n",
    "            train_file_path = self.ingestion_config.train_data_path\n",
    "            os.makedirs(os.path.dirname(train_file_path), exist_ok=True)\n",
    "            train_data.to_csv(train_file_path, index=False, header=True)\n",
    "\n",
    "            test_file_path = self.ingestion_config.test_data_path\n",
    "            os.makedirs(os.path.dirname(test_file_path), exist_ok=True)\n",
    "            test_data.to_csv(test_file_path, index=False, header=True)\n",
    "\n",
    "            logging.info(\"Train & Test datasets saved successfully.\")\n",
    "            return train_data, test_data\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "\n",
    "    def init_data_ingestion(self):\n",
    "        try:\n",
    "            dataframe = self.get_feature_extract_data()\n",
    "            print(dataframe.head())\n",
    "            self.split_data(dataframe)\n",
    "\n",
    "            data_ingestion_artifact = Data_Ingestion_Artifact(\n",
    "                train_file_path=self.ingestion_config.train_data_path,\n",
    "                test_file_path=self.ingestion_config.test_data_path\n",
    "            )\n",
    "            logging.info(\"Data Ingestion completed successfully.\")\n",
    "            return data_ingestion_artifact\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys) \n",
    "        \n",
    "\n",
    "from Network_Security.components.data_ingestion import Data_Ingestion\n",
    "from Network_Security.components.data_validation import Data_validation\n",
    "from Network_Security.entity.config import (Data_ingestion_Config,\n",
    "                                            Data_validation_config) \n",
    "from Network_Security.entity.artifact import (Data_Ingestion_Artifact,\n",
    "                                              Data_validation_Artifact)\n",
    "\n",
    "\n",
    "\n",
    "class Training_Pipeline:\n",
    "    def __init__(self):\n",
    "        self.data_ingestion_config = Data_ingestion_Config()\n",
    "\n",
    "    def start_data_ingestion(self)->Data_Ingestion_Artifact:\n",
    "        data_ingestion = Data_Ingestion(ingestion_config=self.data_ingestion_config)\n",
    "        data_ingestion_artifact = data_ingestion.init_data_ingestion()\n",
    "        return data_ingestion_artifact \n",
    "\n",
    "    def run_pipeline(self)->None:\n",
    "        data_ingestion_artifact = self.start_data_ingestion()\n",
    "        \n",
    "\n",
    "        return None   \n",
    "\n",
    "from Network_Security.pipeline.train_pipeline import Training_Pipeline\n",
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "import sys \n",
    "\n",
    "if __name__ == '__main__':\n",
    "        logging.info('Starting Training Pipeline...')\n",
    "        pipeline = Training_Pipeline()\n",
    "\n",
    "        # Data Ingestion\n",
    "        logging.info('>>> Starting Data Ingestion')\n",
    "        data_ingestion_artifact = pipeline.start_data_ingestion()\n",
    "        logging.info(f'>>> Data Ingestion Completed: {data_ingestion_artifact}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5203ab0",
   "metadata": {},
   "source": [
    "# Data_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ead96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant.__init__.py\n",
    "DATA_VALIDATION_DIR:str = 'data_validation'\n",
    "DATA_VALIDATION_REPORT_DIR:str = 'drift_report'\n",
    "DATA_VALIDATION_REPORT_YAML:str = 'report.yaml'\n",
    "SEHEMA_FILE_PATH = os.path.join('data_schema','column.yaml')\n",
    "\n",
    "# config.py\n",
    "from dataclasses import dataclass \n",
    "from datetime import datetime\n",
    "from Network_Security.constant import *\n",
    "TIMESTAMP = datetime.now().strftime('%m_%d_%Y_%H_%M_%S')\n",
    "\n",
    "@dataclass \n",
    "class NS_Train_Configeration:\n",
    "    artifact_dir:str = os.path.join(ARTIFACTS,TIMESTAMP)\n",
    "    pipeline_dir:str = PIPELINE_DIR\n",
    "    TIMESTAMP:str = TIMESTAMP\n",
    "\n",
    "train_config = NS_Train_Configeration()\n",
    "class Data_validation_config:\n",
    "    data_validation_dir = os.path.join(train_config.artifact_dir,DATA_VALIDATION_DIR)\n",
    "    data_validation_report = os.path.join(data_validation_dir,DATA_VALIDATION_REPORT_DIR,DATA_VALIDATION_REPORT_YAML)\n",
    "\n",
    "# artifact.py\n",
    "@dataclass \n",
    "class Data_validation_Artifact:\n",
    "    validation_status:bool \n",
    "    message_error:str \n",
    "    drift_report_file_path:str\n",
    "\n",
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.constant import SEHEMA_FILE_PATH\n",
    "from Network_Security.utils import read_yaml_file, write_yaml_file\n",
    "from Network_Security.entity.artifact import Data_Ingestion_Artifact, Data_validation_Artifact\n",
    "from Network_Security.entity.config import Data_validation_config\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "from evidently import Report\n",
    "from evidently.presets import DataDriftPreset\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# data_validation.py\n",
    "class Data_validation:\n",
    "    def __init__(self, data_ingestion_artifact: Data_Ingestion_Artifact,\n",
    "                data_validation_config: Data_validation_config):\n",
    "        try:\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_validation_config = data_validation_config\n",
    "            self._schema_yaml = read_yaml_file(file_path=SEHEMA_FILE_PATH)\n",
    "            if self._schema_yaml is None:\n",
    "                raise ValueError(f\"Schema file not loaded or is empty: {SEHEMA_FILE_PATH}\")\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "        \n",
    "    #if number of columns matches schema:\n",
    "    def valid_no_columns(self, dataframe: pd.DataFrame) -> bool:\n",
    "        try:\n",
    "            expected_columns = self._schema_yaml['columns']\n",
    "            status = len(dataframe.columns) == len(expected_columns)\n",
    "            return status\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "\n",
    "    #if all expected columns exist:\n",
    "    def is_column_exists(self, dataframe: pd.DataFrame) -> bool:\n",
    "        try:\n",
    "            missing_num_columns = [col for col in self._schema_yaml['numeric_columns'] if col not in dataframe.columns]\n",
    "            missing_cat_columns = [col for col in self._schema_yaml['categorical_columns'] if col not in dataframe.columns]\n",
    "\n",
    "            if missing_num_columns:\n",
    "                logging.info(f'Missing numeric columns: {missing_num_columns}')\n",
    "            if missing_cat_columns:\n",
    "                logging.info(f'Missing categorical columns: {missing_cat_columns}')\n",
    "\n",
    "            status = not (len(missing_num_columns) > 0 or len(missing_cat_columns) > 0)\n",
    "            return status\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "\n",
    "    def detect_dataset_drift(self, reference_df: pd.DataFrame, current_df: pd.DataFrame) -> bool:\n",
    "        try:\n",
    "            report = Report([DataDriftPreset()],include_tests=\"True\")\n",
    "            report = report.run(reference_data=reference_df, current_data=current_df)\n",
    "            report.save_html(\"data_drift_report.html\")\n",
    "            json_report = report.json()\n",
    "            report_dict = json.loads(json_report)\n",
    "            write_yaml_file(\n",
    "                file_path=self.data_validation_config.data_validation_report,\n",
    "                content=report_dict)\n",
    "            \n",
    "            n_features = sum(1 for m in report_dict[\"metrics\"] if \"ValueDrift\" in m[\"metric_id\"])\n",
    "            drift_metric = next(m for m in report_dict[\"metrics\"] if \"DriftedColumnsCount\" in m[\"metric_id\"])\n",
    "            n_drifted_features = drift_metric[\"value\"][\"count\"]\n",
    "            # Dataset drift status\n",
    "            drift_status = n_drifted_features > 0\n",
    "            print(n_features, n_drifted_features, drift_status)\n",
    "            logging.info(f\"{n_drifted_features}/{n_features} features show drift.\")\n",
    "            return drift_status    \n",
    "        except Exception as e:\n",
    "            logging.info(f\"Error in dataset drift detection: {e}\")\n",
    "            raise NetworkSecurityException (e,sys)\n",
    "  \n",
    "    # Static method to read CSV\n",
    "    @staticmethod\n",
    "    def read_data(file_path: str) -> pd.DataFrame:\n",
    "        return pd.read_csv(file_path)\n",
    "    \n",
    "    def init_data_validation(self) -> Data_validation_Artifact:\n",
    "        try:\n",
    "            valid_message_error = []\n",
    "            # Read train and test data\n",
    "            train_data = self.read_data(self.data_ingestion_artifact.train_file_path)\n",
    "            test_data = self.read_data(self.data_ingestion_artifact.test_file_path)\n",
    "            # train data\n",
    "            if not self.valid_no_columns(train_data):\n",
    "                valid_message_error.append('Error: Column Mismatch in train data')\n",
    "            if not self.is_column_exists(train_data):\n",
    "                valid_message_error.append('Error: Missing columns in train data')\n",
    "            #test data\n",
    "            if not self.valid_no_columns(test_data):\n",
    "                valid_message_error.append('Error: Column Mismatch in test data')\n",
    "            if not self.is_column_exists(test_data):\n",
    "                valid_message_error.append('Error: Missing columns in test data')\n",
    "\n",
    "            # Drift detection\n",
    "            validation_status = len(valid_message_error) == 0\n",
    "            if validation_status:\n",
    "                drift_status = self.detect_dataset_drift(train_data, test_data)\n",
    "                if drift_status:\n",
    "                    valid_message_error.append('Drift detected')\n",
    "                else:\n",
    "                    valid_message_error.append('Drift not detected')\n",
    "            else:\n",
    "                logging.info(f'Validation errors: {valid_message_error}')\n",
    "\n",
    "            #Create artifact\n",
    "            data_validation_artifact = Data_validation_Artifact(\n",
    "                validation_status=validation_status,\n",
    "                message_error=valid_message_error,\n",
    "                drift_report_file_path=self.data_validation_config.data_validation_report\n",
    "            )\n",
    "            return data_validation_artifact\n",
    "\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e, sys)\n",
    "\n",
    "# train_pipeline.py \n",
    "from Network_Security.components.data_ingestion import Data_Ingestion\n",
    "from Network_Security.components.data_validation import Data_validation\n",
    "from Network_Security.entity.config import (Data_ingestion_Config,\n",
    "                                            Data_validation_config) \n",
    "from Network_Security.entity.artifact import (Data_Ingestion_Artifact,\n",
    "                                              Data_validation_Artifact)\n",
    "class Training_Pipeline:\n",
    "    def __init__(self):\n",
    "        self.data_ingestion_config = Data_ingestion_Config()\n",
    "        self.data_validation_config = Data_validation_config()\n",
    "\n",
    "\n",
    "    def start_data_ingestion(self)->Data_Ingestion_Artifact:\n",
    "        data_ingestion = Data_Ingestion(ingestion_config=self.data_ingestion_config)\n",
    "        data_ingestion_artifact = data_ingestion.init_data_ingestion()\n",
    "        return data_ingestion_artifact \n",
    "    \n",
    "    def start_data_validation(self, data_ingestion_artifact: Data_Ingestion_Artifact) -> Data_validation_Artifact:\n",
    "        data_valid = Data_validation(data_ingestion_artifact=data_ingestion_artifact,\n",
    "                                    data_validation_config=self.data_validation_config)\n",
    "        data_validation_artifact = data_valid.init_data_validation()\n",
    "        return data_validation_artifact\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run_pipeline(self)->None:\n",
    "        data_ingestion_artifact = self.start_data_ingestion()\n",
    "        data_validation_artifact=self.start_data_validation(data_ingestion_artifact)\n",
    "\n",
    "        return None\n",
    "\n",
    "# app.py \n",
    "from Network_Security.pipeline.train_pipeline import Training_Pipeline\n",
    "from Network_Security.logging.logger import logging\n",
    "from Network_Security.exception.exception import NetworkSecurityException\n",
    "import sys \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logging.info('Starting Training Pipeline...')\n",
    "        pipeline = Training_Pipeline()\n",
    "\n",
    "        # Data Ingestion\n",
    "        logging.info('>>> Starting Data Ingestion')\n",
    "        data_ingestion_artifact = pipeline.start_data_ingestion()\n",
    "        logging.info(f'>>> Data Ingestion Completed: {data_ingestion_artifact}')\n",
    "\n",
    "        # Data Validation\n",
    "        logging.info('>>> Starting Data Validation')\n",
    "        data_validation_artifact = pipeline.start_data_validation(data_ingestion_artifact)\n",
    "        logging.info(f'>>> Data Validation Completed: {data_validation_artifact}')\n",
    "\n",
    "        logging.info('Pipeline finished successfully')\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise NetworkSecurityException(e, sys)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
